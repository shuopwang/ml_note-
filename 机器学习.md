# 机器学习面试

最大似然估计的目的：假设样本是独立由真实数据分布p（x）生成，利用已知的样本结果，反推最大概率导致这样结果的参数值。最大化似然参数，和最小化KL散度是相同的。

### 1. LR和SVM区别和联系

模型的损失函数由各自的响应变量y的概率分布决定, 线性回归假设y为正态分布，LR假设为类别是01分布，

几率：一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值。LR的几率即为wx。

LR模型直接对分类可能性P\(Y\|X\)进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题。LR将连续特征离散化，离散化也可以提高模型的鲁棒性，提高它的特征表示能力，如果还是连续值，那么每个字段只能用一个w来刻画，离散化成为N个之后，相当于为模型增加了非线性。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。

LR模型不是仅预测出类别，而是可得到近似概率预测，这对于许多需利用概率辅助决策的任务很有用  

LR为什么一般采用离散特征：
0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。  

对数几率函数是任意阶可导的凸函数，有很好的数学性质，现有很多数值优化算法均可直接用于求取最优解。凸函数定义

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/149a729ffa207aca4e585b3863b56524602a497d)

LR的损失函数为什么要用最大似然：

1 想让每个样本的预测都要得到最大的概率2最大似然之后取对数相当于对数损失函数，求解速度快且为凸函数有全局最小值3采用平方损失的梯度下降速度慢，并且不是凸函数

相似：

**第一，LR和SVM都是分类算法。尽管，lr输出的为label=1的概率**  
**第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。**  
**第三，LR和SVM都是监督学习算法。**

不同：

**第一，本质上是其loss function不同。**

​逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。

支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面。令两类之间的间隔越大越好。

**第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。**

SVM的特点由对偶之后的KTT条件引出，只考虑support vector即离分类平面距离为1的点，这也是它可以支持核函数的重要原因

**逻辑回归考虑全局（远离的点对边界线的确定也起作用）**

线性SVM不直接依赖于数据分布，分类平面不受一类点影响，由于只需要考虑support point；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing

**第三，​线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响**

SVM依赖数据表达的距离测度，因为可以自然想到，在找support point的时候还是需要有一定的距离的衡量的，如果在高纬度的向量时这个比较难搞。否则准确率会受到影响

**第四，SVM的损失函数就自带正则！！！（损失函数中的1/2\|\|w\|\|^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**  
**泛化性能**：LR如果不使用正则化项的话，很容易过拟合，因为一旦特征多了，模型就会变得很复杂。可以手动减少特征或者使用正则化项。SVM的模型复杂度和特征数目关系不大，所以过拟合的风险没那么大。

### 2. XGBOOST 与GBDT

这两个模型都使用随机特征子集, 来生成许多单个的树.

XGB优点：用二阶泰勒展开来拟合残差。带来更快的拟合，也大大缩减了生成树的规模，减少了运行时间。XGBoost比GBDT多添加了以树复杂度构成的正则化项，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合。gamma叶节点进一步切分的最小损失下降的阈值\(超过该值才进一步切分\)

xgboost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性. xgboost利用梯度优化模型算法, 样本是不放回的\(想象一个样本连续重复抽出,梯度来回踏步会不会高兴\). 但xgboost支持子采样, 也就是每轮计算可以不使用全部样本,也可以不使用全部特征。

并行化加速运算，这个是xgb和gbdt都会有的。步骤是在建树的过程中，由于每个特征值计算最佳分割值是相互独立的，故可以对特征进行平分，再同时进行计算。在面对连续值的情况下，xgb可以用近似直方图利用二阶导数来进行加权求分位点对分位点进行计算增益来近似运算。

xgb有一个缩减的机制，可以减少每个树带来的影响。再有缺失值的情况下，近似直方图只考虑非零项，将缺失项分别进行计算其增益，对缺失值选择最高值对应的分裂值和得分值。

### 3. Bagging 与Boosting

对于Bagging算法来说，相当于一个并联机制。或称为bootstrap aggregating。从初始的训练集中随机取出的n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后可得到一个预测函数序列h\_1，⋯ ⋯h\_n ，最终的预测函数H对分类问题采用投票方式。由于我们会并行地训练很多不同的分类器的目的就是降低这个方差\(variance\) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias\),所以我们会采用深度很深甚至不剪枝的决策树。RF  
对于Boosting来说，相当于串联。使用全部数据。每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。XGB和GBDT。弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。

RF和GBDT都是基于cart，用基尼系数来作为分裂指标

#### 特征重要性选择方法：仅针对于bagging

1）对每一颗决策树，选择相应的袋外数据（out of bag，OOB）​计算袋外数据误差，记为errOOB1.

2）随机对袋外数据OOB所有样本的特征A加入噪声干扰（可以随机改变样本在特征A处的值），再次计算袋外数据误差，记为errOOB2。

3）​假设森林中有N棵树，则特征A的重要性=∑（errOOB2-errOOB1）/N。这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。

### 4. L1和L2的区别 

正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。正则化，让模型变得简单，加入了模型的先验假设。先验知识可以防止过拟合。  
1、L1是Lasso Regression，表示向量中每个元素绝对值的和：L1范数的解通常是稀疏性的，倾向于选择数目较少的一些非常大的值或者数目较多的insignificant的小值。 通过selectfrommodel 调整这个的阈值，阈值作用在是否选择这个特征，和lasso的alpha值，alpha即控制惩罚因子的量。当l1不可导时，可以规定sgn（0）=0  
2、L2是岭回归，Ridge Regression，是欧氏距离也就是平方和的平方根。L2范数越小，可以使得w的每个元素都很小，接近于0，但L1范数不同的是他不会让它等于0而是接近于0。   
3、L1正则化的w可取的值是转置的方形，L2对应的是圆形。这样损失函数l（w）的最小值更容易在L1对应的边角上取得，从而这些维度变成0了。

4、L1是默认参数为拉普拉斯分布，L2是默认参数为高斯分布

### 5. **为什么朴素贝叶斯如此“朴素”？**

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

### 6. EM

有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的。不用梯度下降法的原因在于难以直接求导，即使是引入隐藏变量之后，求的梯度项数将随着因变量的数目指数上升。EM通过迭代，不断求解下界的极大化，来逐步求解对数似然函数极大化（对应模型参数个数可能有多个）Jensen不等式证明其单调有界，所以EM收敛。f\(E\(x\)\)&lt;=E\(f\(x\)\)

EM算法一般分为2步：  
E步：选取一组参数，求出在该参数下隐含变量的条件概率值；M：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。

### 7.过拟合方法

**防止过拟合的方法**   
　　过拟合的原因是算法的学习能力过强，参数太多模型复杂；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。   
　　处理方法有：

* a.早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练
* b.数据集扩增：原有数据增加、原有数据加随机噪声、重采样
* c.正则化
* d.交叉验证
* e.特征选择/特征降维

### 8. 数据归一化

1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度

2\)一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）令数据的尺度保持一致。

z-score归一化:x-mean\(s\)/std\(x\)

该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕

**哪些机器学习算法不需要做归一化处理？** 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，决策树模型大都是基于信息量上度量的关心的是概率值，特征量纲不同不会造成太大的印象，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

### 9. 最优化的方法

损失函数为凸函数时存在全局最优点。

梯度下降以及其几个变种，优点特点。牛顿法收敛速度快，但是计算复杂，设计到二阶导数矩阵的计算。

共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一

1）随机梯度下降  
优点：可以一定程度上解决局部最优解的问题  
缺点：收敛速度较慢  
2）梯度下降  
优点：容易陷入局部最优解  
缺点：收敛速度较快  
3）mini\_batch梯度下降  
综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。  
4）牛顿法  
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。  
5）拟牛顿法  
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

### 10. 损失函数

0-1 损失

平方损失

log损失：交叉熵。lr损失函数

绝对损失

这些损失函数的建立是为了经验风险最小化，减少bias。加上正则项则是为了结构风险最小化，防止因为模型过于复杂造成的过拟合。

### 11. 样本不均衡

欠采样，设置类别loss权重，维度低用smote生成数据样本，重复数量少的样本易过拟合

用boosting, c1根据原始数据集训练，c2根据l1分类正确：l1分类错误采样1：1的数据集来训练，c3利用c2分类错误的的样本训练，投票vote，c2和c3均为false才为false

采用的指标一般是F1 和AUC

logloss和auc的区别：logloss主要是评估是否准确的，auc是用来评估是把正样本排到前面的能力，评估的方面不一样。对预测的pctr，乘以一个倍数，auc是不变的，因为相互的排序关系没有变，但是logloss会变。auc 更多的关注的是排序的结果。logloss 则是越小越好

对多类进行聚类，对聚类结果抽取样本，组成和少类大小相似的样本

设置阈值，损失函数的时候给少类提升weight

SMOTE算法:对少数类样本找到某个样本xi的k个邻居。从这个k个邻居中随机选择样本xj，在生成一个0到1之间的随机数。新样本=xi+随机数\(xj-xi\)相当于在差值法

### 12. Kmeans 复杂度

Kd-Tree进行加速

1。从k维数据集合中选择具有最大方法的维度l，选择中值作为划分，左边小于等于它，右边大于它,得到&lt;k,m&gt;

2. 对两个子集合重复上述操作。直至不能划分

找最近的节点时：通过dfs回溯法，每次更新。

时间复杂度：O\(tKmn\)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数空间复杂度：O\(\(m+K\)n\)，其中，K为簇的数目，m为记录数，n为维数

Kmeans 中k值的选择，利用轮廓系数。簇内不相似度平均距离来衡量a，簇间不相似度b，簇间不相似度利用的是所有别的簇到这个点的最小距离。s越大说明聚类效果越好。s是所有点的平均

```text
if a<b:
s=1-(a/b)
else:
s=(b/a)-1
```

### 13.各个距离函数的选择与特点



### 14**解释对偶的概念**

一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

### 15.**如何进行特征选择？** 

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解。一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的  
常见的特征选择方式：  
1. 去除方差较小的特征。方差越大，信息量越大:对连续型随机变量X ,熵H\(X\)随标准差σ的增加而增加。对离散型随机变量X ,熵与方差无关。skelarn中有VarianceThreashold，当不设阈值时可以去除取值完全相同的特征  
2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。  
3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。  
4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

5.Pearson 相关系数，从协方差而来，多除了分母标准差，用以消除量纲的影响。相关系数的取值范围为\[-1,1\]。当相关系数为1时，成为完全正相关；当相关系数为-1时，成为完全负相关；相关系数的绝对值越大，相关性越强；相关系数越接近于0，相关度越弱。所以可以利用它来去除相关度比较大的特征。相关系数为0是两变量独立的必要非充分条件。Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系5。5对于非线性关系利用互信息来衡量，互信息即是

类别特征本身**有顺序**（例：优秀、良好、合格、不合格），那么可以保留单列自然数编码。如果类别特征**没有明显的顺序**（例：红、黄、蓝）预处理时采用one-hot编码：那针对这种情况最简单的处理方式是将不同的类别映射为一个整数，比如男性是0号特征，女性为1号特征。这种方式最大的优点就是简单粗暴，实现简单。那最大的问题就是在这种处理方式中，各种类别的特征都被看成是有序的，这显然是非常不符合实际场景的。1.能够处理非连续型数值特征。 2.在一定程度上也扩充了特征。将离散特征的取值扩展到了空间上。可以会让特征之间的距离计算更加合理

数据缺失：

缺失值较少处理方法：连续值用中位数，离散值用众数。取一个异常值表示。

缺失值较多且不重要，直接删除。进行对照比对，看结果影响。

### 16.**SVD和PCA**

　　PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵，奇异值分解协方差矩阵。协方差矩阵就是利用内急进行相似性的度量。消除量纲之后就是中心化的夹角余弦值相当于相关系数。是因为在考虑投影之后的方差最大时协方差矩阵要求的，所以可以推导需要做中心化数据。当投影之后的矩阵维度和原矩阵一致的时候，相当于起到一个降噪的作用。

投影之后的协方差矩阵=P\*C\*transpose\(P\)

$$
Av = lambda    v  最后的v为矩阵A的特征向量，lambda为v对应的特征值
$$

A=Q∑transpose\(Q\) Q为特征向量。

### 17.HMM

前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。  
Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；估计A，B，pai的参数。根据P（Y\|X）来更新隐含变量Y的期望，用Y的期望来代替Y来求解新的模型参数

EM：init: 随机给参数pai，A，B赋值，满足概率和为1的约束

E：这个公式时通过在对P\(I,O\|siga\)\*P\(I,O\|siga^\)进行求极值的时候，通过引入拉格朗日算子，对各个参数求偏导得到的。求出gamma观察序列O在估计的参数下，在时间t位于状态si，t+1位于状态sj的概率。

![](.gitbook/assets/image%20%2833%29.png)

以及xi在时间步t时位于状态si的概率

![](.gitbook/assets/image%20%2819%29.png)

M: 根据上述这两项来更新，新的pai就是xi在时间步为0的时候，A就分子就是i-j的期望次数那就是所有时间步上由i-j的和，分母就是xi在状态为i之和，B的分子可能到达j状态乘时间步下表现为k的

![](.gitbook/assets/image%20%2829%29.png)

  
解码问题，有两种方法，一种是近似方法，近似方法是每次取当前步骤下概率最大，一种是维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。O\(T\*N^2\)

在时刻t下隐藏状态为i的，可以有多条路径，其中概率最大为sigma

![](.gitbook/assets/image%20%288%29.png)

定义为时刻t状态为i的所有单个路径中概率最大的路径的前i-1个节点

![](.gitbook/assets/image%20%2820%29.png)

在递推的过程中，在每一个时间步t下，对于每个可能的状态i（共有N个）都需要考虑到达当前状态并有当前的output时候的概率最大的概率值，记录以及到达状态i的情况下该点的前一个状态j。当我们走到了最后，找到了最大概率，因为最后一步也是会有N个可能的sigma k，从而得到，最有可能的状态为k，然后再根据记录的fi的值来得到上一个时间步下状态为k的最后可能的状态。

### 18. bias and variance

交叉验证  
High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征  
High Variance解决方案：bagging、简化模型、降维

### 20.多标签问题

Binary-relevance:将多标签问题分解成多个分类问题。问题在于未考虑标签之间的相似性。

Label combination：将多个标签信息视为一种新的标签，从而将问题转化为mutlilabel问题。规模大power\(2,L\)

衡量手段：Hamming score，各个标签与ground truth之间的差异。J index：比较相同项在所有项中的占比情况。

流式机器学习online learning

流式问题中需要有一个alarm来检测何时有数据变化，这个数据分布变化就叫做概念漂移，并miniimizing预测误差

Hoeffding Tree，基础为C4.5，不同在于分裂节点前，有一个阈值来控制，起到预剪枝的作用，如果最高的属性的信息增益率和次高的信息增益率低于hoeffding bound那么就不分裂。因为分裂节点是需要G\(A\)-G\(any\)&gt;0，才能证明肯定是最好的节点，如果想要在分布无穷大的DS中成立只要让G\(A\)-G\(any\)-hoeffding bound&gt;0.

### 19. Hive

Hive使用类sql语句进行相关操作，称为HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。

Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询

**count\(\*\)包括了所有的列，相当于行数**，在统计结果的时候，**不会忽略列值为NULL    
count\(1\)**包括了忽略所有列，用1代表代码行，在统计结果的时候，**不会忽略列值为NULL    
count\(列名\)**只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，**即某个字段值为NULL时，不统计**。

### 20.矩阵分解时的梯度下降。CUDA  

简单的说，gpu由上千个流处理器(core)作为运算器。执行采用单指令多线程(SIMT)模式。在cuda编程中有grids、blocks、threads三个维度，大量threads组成一个block，大量blocks组成一个grid。每个block内的所有threads执行相同的操作。利用函数kernel调用执行。blockDim.x、blockIdx.x、threadIdx.x分别表示每个block有多少thread、当前block的索引、当前thread的索引。    

### 21.JAVA

ArrayList、LinkedList、Vector都实现了List的接口。ArrayList是为可变数组实现，每个ArrayList实例都有一个容量（Capacity），即用于存储元素的数组的大小。这个容量可随着不断添加新元素而自动增加。List 接口的链接列表实现。实现所有可选的列表操作，并且允许所有元素（包括 null）。LinkedList是为双向链表实现的，添加、删除元素的性能比ArrayList好，但是get/set元素的性能较差。Vector （类似于ArrayList）但其是同步的，开销就比ArrayList要大。如果你的程序本身是线程安全的，那么使用ArrayList是更好的选择。 Vector和ArrayList在更多元素添加进来时会请求更大的空间。Vector每次请求其大小的双倍空间，而ArrayList每次对size增长50%.

链表和数组的区别：数组查找速度快，随机读取快。但是插入和删除的效率低，浪费内存，必须时连续的内存使用空间。数组大小固定不能动态扩展。链表，插入删除速度快。内存利用率高。扩展灵活。查找麻烦

抽象类：抽象类不一定包含抽象方法，有抽象方法的类一定时抽象类。无法实例化,有构造函数，并且抽象类中有默认方法去实现。需要。子类只能继承一个抽象类extends，子类同时可以实现多个接口。非抽象方法必须实现所有申明方法的实现，抽象类中的抽象方法只申明不实现。public,protected,default。可以有main 方法。添加新方法时，在子类中不需要新加代码，可以自动继承。子类在继承父类时发现了抽象方法时，必须实现它。

接口：抽象方法的集合，接口中的方法为隐式抽象，不需加abstract关键字。如果实现某个接口，就要确保实例化接口里面的方法implements，只能为public。不能有main方法。接口只能继承接口。由于接口方法没法在接口中实例化，所以在接口中新加方法需要修改所有代码。接口继承接口也为extends关键字

因为默认的equals方法是Object的方法，比较的是内存地址；而默认的hashcode方法返回的是对象的内存地址转换成的一个整数，实际上指的的也是内存，两个方法可以理解为比较的都是内存地址。我们在定义类时，**我们经常会希望两个不同对象的某些属性值相同时就认为他们相同**。JAVA要求equals\(\)相等，hashcode\(\)一定相等，如果你重载了equals，比如说是基于对象的内容实现的，而保留hashCode的实现不变，那么很可能某两个对象明明是“相等”，而hashCode却不一样。

hashmap和hashtable：

hashmap是非同步的，意味着多个线程无法共享hashmap，可以接受键值为null，如果实在单一线程的情况，使用hashmap性能要好过hashtable。有containsvalue和containskey方法。继承自抽象map。不允许有重复的key。HashMap计算hash对key的hashcode进行了二次hash，以获得更好的散列值，然后对table数组长度取摸

Hashtable是同步的，有contains方法。继承自字典对象。Hashtable计算hash是直接使用key的hashcode对table数组的长度直接进行取模

### HashMap和Hashtable的底层实现都是数组+链表结构实现 {#hashmap和hashtable的底层实现都是数组链表结构实现}

添加、删除、获取元素时都是先计算hash，根据hash和table.length计算index也就是table数组的下标，然后进行相应操作。put过程是先计算hash然后通过hash与table.length取摸计算index值，然后将key放到table\[index\]位置，当table\[index\]已存在其它元素时，会在table\[index\]位置形成一个链表，将新添加的元素放在table\[index\]，原来的元素通过Entry的next进行链接，这样以链表形式解决hash冲突问题，当元素数量达到临界值\(capactiy\*factor\)时，则进行扩容，是table数组长度变为table.length\*2。Entry&lt;K,V&gt; e， 通过equals方法得到正确的节点。

equals方法相当于在内存空间中两者的存放地址是否相同。

HashCode方法得到key 在链表中存储位置。

如果超过了负载因子，就会创建原来hashmao大小的两倍的bucket数

HashSet不是key value结构，仅仅是存储不重复的元素，相当于简化版的HashMap，只是包含HashMap中的key而已

TreeMap能够把它保存的记录根据键排序，默认是按升序排序，也可以指定排序的比较器。当用Iteraor遍历TreeMap时，得到的记录是排过序的。TreeMap的键和值都不能为空。

同步：意味着一次仅有一个线程能够更改hashtable

哈希冲突：指的就是不同的数据经过哈希算法之后得到的相同的key值。hashmap采用链表法。

判断一个内存空间是否符合垃圾收集标准有两个：一个是给对象赋予null，再也没有调用，另一个是给对象赋予了新值。

JVM内存分区：

线程计数器：

堆区：存放对象和数组，new的对象内存都在此分配。new Student的Student实力

栈区：栈与线程计数器一样，生命周期与线程的生命周期同步。每次方法执行就会同时创建一个栈帧，用于存储局部变量和操作数栈。存储Student类实例的变量，变量存储的是此实例的地址赋值引用。局部变量存世位置。

本地方法栈：本地方法执行进入本地方法栈

方法栈：和堆区一样，都是线程共享。存放JVM加载的类型信息，如类型基本信息，常量，方法表等等

float和Float的区别，Float是一个类型，这个变量是对这个类型对象的引用，float是基本数据类型是原始类型。在往ArrayList, HashMap中放东西，只能放对象。

内存分配：c语言

栈区:自动分配释放，存放函数的参数值，局部变量的值

堆区：有程序员分配释放，若不释放，结束时由系统释放，用malloc申请内存空间的时候使用new申请内存。堆区总大小为机器的虚拟内存大小

malloc是c语言，new是c++

malloc返回是void，new返回带类型的指针

malloc只负责内存分配，new会在分配内存的同时自动调用类的构造函数

静态区：初始化后的全局变量和静态变量还有常量在一块，未初始化的全局变量和静态变量在相邻的另一块区域，由系统释放

文字常量区：字符串常量

程序代码区



垃圾回收：可以防止内存泄漏，发现无用信息对象，回收无用对象占用空间。

1.引用计数：对象被引用+1，取消饮用-1。到0回收。无法解决循环引用

2. 可达性分析算法：用引用关系的就画点，从根节点开始，寻找引用，所有可达的引用寻找完毕后剩余对象即为无用节点

标记清除：直接回收会造成内存碎片。

标记整理算法，将存活对象往空闲端移动，并更新对应的指针。

复制算法：将内存按容量分为凉快，每次只用一块，一块用完，就将还存活的对象复制到另外一块上

GC：根据不同生命周期将对象分为三种，年轻代，年老代，持久代。刚开始生成的都放在young，历经n轮不死的放在年老代，一直都会存在的放在持久代，不同的时间周期用不同的方法进行回收。新生代用复制，老年代用标记整理，cms用标记清理

内存泄露：内存申请之后，用完没有释放，造成可用的内存越来越少。根本原因在于：长生命周期对象持有短生命周期对象的引用。导致短生命周期无法回收。    

内存溢出：用户实际数据长度超过了申请内存空间的大小。

通常时内存泄露导致了内存溢出

### cs基础

Linux中进程与线程的通信方式：  
Linux系统中的进程间通信方式主要以下几种:  
同一主机上的进程通信方式：  
* UNIX进程间通信方式: 包括管道(PIPE)：速度慢，容量有限，只有父子进程能通讯, 有名管道(FIFO)：任何进程间都能通讯，但速度慢, 和信号(Signal)
* System V进程通信方式：包括信号量(Semaphore)：不能传递复杂消息，只能用来同步, 消息队列(Message Queue), 和共享内存(Shared Memory)：能够很容易控制容量，速度快  
网络主机间的进程通信方式  
* RPC: Remote Procedure Call 远程过程调用  
* Socket: 当前最流行的网络通信方式, 基于TCP/IP协议的通信方式.  

线程之间通信：目的是为了保持线程之间的同步。利用锁机制：包括互斥锁、条件变量、读写锁。  
互斥锁提供了以排他方式防止数据结构被并发修改的方法。  
使用条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。  
读写锁允许多个线程同时读共享数据，而对写操作是互斥的。  

虚拟内存：虚拟内存别称虚拟存储器（Virtual Memory）。电脑中所运行的程序均需经由内存执行，若执行的程序占用内存很大或很多，则会导致内存消耗殆尽。为解决该问题，Windows中运用了虚拟内存技术，即匀出一部分硬盘空间来充当内存使用。当内存耗尽时，电脑就会自动调用硬盘来充当内存，以缓解内存的紧张。若计算机运行程序或操作所需的随机存储器(RAM)不足时，则 Windows 会用虚拟存储器进行补偿。它将计算机的RAM和硬盘上的临时空间组合。当RAM运行速率缓慢时，它便将数据从RAM移动到称为“分页文件”的空间中。

线程与进程：一个程序至少有一个进程,一个进程至少有一个线程. 线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。线程冲突，多个线程访问了相同的资源，并同时都有写操作。

多进程：
multiprocessing import Process
传入function和function用到的参数
start()启动执行函数join()等待子进程结束后再继续往下运行，通常用于进程间的同步。
启动大量子进程通过进程池的方法，调用join()之前必须先调用close(). Pool有默认执行多少个进程的数目大小

多线程：
import threading
传入函数，并创建相应的thread实例，然后调用start()开始执行。多进程中同一个变量各自拷贝再每个进程中，互不影响。多线程中，所有变量有所有线程共享。给线程一个线程锁就可以保证同一个时刻最多只有一个线程持有锁，不会造成修改的冲突。
threading.lock()
#只有一个线程可以获取锁
lock.acquire()
  try:
    ...
  finally:
#修改完成之后释放锁
lock.release()

在python中利用multiprocess而不使用multi-thread，因为python的多线程在有同时写操作时需要加上线程锁才可以保证线程安全。GIL，Global Interpreter Lock。这样就相当于一个串行的过程。全局解释器锁（英语：Global Interpreter Lock，缩写GIL），是计算机程序设计语言解释器用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。[1]即便在多核心处理器上，使用 GIL 的解释器也只允许同一时间执行一个线程

JAVA实现多线程的方法。继承Thread，重写run方法。利用start\(\)方法启动线程。或者由Runnable实现接口。

TCP三次握手：第一次握手：建立链接时，客户端发送同步序号 syn和ACK以及一部分数据x到服务器段等待确认。第二次握手：服务器收到报文请求，由SYN直到客户端建立连接请求，向客户端发送ack=x+1，以及一个syn的包，seq=Y，服务器段进入SYN\_RCVD状态。第三次握手：客户端收到服务器SYN+ACK包，向服务器发送确认包ACK=y+1  
TCP的滑动窗口主要有两个作用，一是提供TCP的可靠性，二是提供TCP的流控特性。  

const:常量的在超出作用域的时候会被释放，static不会，只是不能访问

static:修饰静态变量，静态函数。作用域限制于包含它的文件中。在类内部声明，在类的外部定义和初始化。存放在静态区，在main函数执行前被加载。static 局部变量每次调用的初始值为上一次调用的值。

const常量在类内部声明，定义只能在构造函数中初始化进行。不能被修改。const离谁近，谁不能被修改。const在最前面则将其后移一位

一、面向对象的方法概述：
对象是类的实际例子
1、 抽象：面向对象的软件开发方法的主要特点之一就是采用了数据抽象的方法来构建程序的类及对象。   
2、 封装：封装是一种信息隐蔽技术就是利用抽象数据类型将数据和基于数据的操作封装在一起。用户只能看到对象的封装界面信息对象的内部细节对用户是隐蔽的
3、 继承 继承是指新的类可以获得已有类称为超类、基类或父类的属性和行为称新类为已有类的派生类也称为子类。在继承过程中派生类继承了基类的特性包括方法和实例变量。派生类也可修改继承的方法或增加新的方法使之更适合特殊的需要。
4、单继承 任何一个派生类都只有单一的直接父类类层次结构为树状结构。 
5、多继承 一个类可以有一个以上的直接父类类层次结构为网状结构设计及实现比较复杂。 
6、多态：多态是指一个程序中同名的不同方法共存主要通过子类对父类方法的覆盖来实现。不同类的对象可以响应同名的消息(方法) 具体的实现方法却不同。
多态的前提，1.存在继承关系，2.子类重写父类方法，3.父类数据类型引用指向子类对象。
Parent p = new Child();
当使用多态方式调用方法时，首先检查父类中是否有该方法，如果没有，则编译错误；如果有，再去调用子类的同名方法，方法中参数也需要相同。
多态之后无法使用子类特有的属性和方法。当使用子类方法时需要强制转化成子类类型。不必为派生类写功能调用，只需要对抽象基类进行处理，提高程序的复用性。复用相同的接口，实现不同操作。指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。

### 22 概率题

p概率生成0，1-p概率生成1，求等概率生成1-n

先做一个等概率生成0和1，两个rand，两者一个为01，10时一个为1一个为0。n个数用二进制表示需要多少个1表示调用它。

两个大文件问题。

遍历a，对每个对象求取hash%1000，分别存储到小文件上，b利用相同的思想做。从而只需要比较an和bn中的数据相同

